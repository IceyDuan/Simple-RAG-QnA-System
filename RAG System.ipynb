{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "49f78c16",
   "metadata": {
    "id": "49f78c16"
   },
   "source": [
    "# Homework 4 (Duan Linjia; e1373960)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97240cf2",
   "metadata": {
    "id": "97240cf2"
   },
   "source": [
    "## Construction of a simple RAG"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lK8ZSmXQP1xs",
   "metadata": {
    "id": "lK8ZSmXQP1xs"
   },
   "source": [
    "### (1) Install and Import Libiary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ynWyEBHgP_ZO",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "collapsed": true,
    "id": "ynWyEBHgP_ZO",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "b15b8353-2dd9-4b02-c6f1-6c81d53adeda"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting langchain\n",
      "  Using cached langchain-0.3.23-py3-none-any.whl.metadata (7.8 kB)\n",
      "Collecting openai\n",
      "  Using cached openai-1.75.0-py3-none-any.whl.metadata (25 kB)\n",
      "Collecting faiss-cpu\n",
      "  Using cached faiss_cpu-1.10.0-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (4.4 kB)\n",
      "\u001b[33mWARNING: Retrying (Retry(total=4, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ProtocolError('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))': /simple/pandas/\u001b[0m\u001b[33m\n",
      "\u001b[0mCollecting pandas\n",
      "  Using cached pandas-2.2.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (89 kB)\n",
      "Collecting numpy==1.24.3\n",
      "  Using cached numpy-1.24.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.6 kB)\n",
      "Collecting scikit-learn\n",
      "  Using cached scikit_learn-1.6.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (18 kB)\n",
      "Collecting streamlit\n",
      "  Using cached streamlit-1.44.1-py3-none-any.whl.metadata (8.9 kB)\n",
      "Collecting langchain_community\n",
      "  Using cached langchain_community-0.3.21-py3-none-any.whl.metadata (2.4 kB)\n",
      "Collecting langchain_ollama\n",
      "  Using cached langchain_ollama-0.3.2-py3-none-any.whl.metadata (1.5 kB)\n",
      "Collecting tiktoken\n",
      "  Using cached tiktoken-0.9.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
      "Collecting langchain-openai\n",
      "  Using cached langchain_openai-0.3.14-py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting langchain-core<1.0.0,>=0.3.51 (from langchain)\n",
      "  Using cached langchain_core-0.3.54-py3-none-any.whl.metadata (5.9 kB)\n",
      "Collecting langchain-text-splitters<1.0.0,>=0.3.8 (from langchain)\n",
      "  Using cached langchain_text_splitters-0.3.8-py3-none-any.whl.metadata (1.9 kB)\n",
      "Collecting langsmith<0.4,>=0.1.17 (from langchain)\n",
      "  Using cached langsmith-0.3.32-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting pydantic<3.0.0,>=2.7.4 (from langchain)\n",
      "  Using cached pydantic-2.11.3-py3-none-any.whl.metadata (65 kB)\n",
      "Collecting SQLAlchemy<3,>=1.4 (from langchain)\n",
      "  Using cached sqlalchemy-2.0.40-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.6 kB)\n",
      "Collecting requests<3,>=2 (from langchain)\n",
      "  Using cached requests-2.32.3-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting PyYAML>=5.3 (from langchain)\n",
      "  Using cached PyYAML-6.0.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.1 kB)\n",
      "Collecting anyio<5,>=3.5.0 (from openai)\n",
      "  Using cached anyio-4.9.0-py3-none-any.whl.metadata (4.7 kB)\n",
      "Collecting distro<2,>=1.7.0 (from openai)\n",
      "  Using cached distro-1.9.0-py3-none-any.whl.metadata (6.8 kB)\n",
      "Collecting httpx<1,>=0.23.0 (from openai)\n",
      "  Using cached httpx-0.28.1-py3-none-any.whl.metadata (7.1 kB)\n",
      "Collecting jiter<1,>=0.4.0 (from openai)\n",
      "  Using cached jiter-0.9.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.2 kB)\n",
      "Collecting sniffio (from openai)\n",
      "  Using cached sniffio-1.3.1-py3-none-any.whl.metadata (3.9 kB)\n",
      "Collecting tqdm>4 (from openai)\n",
      "  Using cached tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
      "Collecting typing-extensions<5,>=4.11 (from openai)\n",
      "  Using cached typing_extensions-4.13.2-py3-none-any.whl.metadata (3.0 kB)\n",
      "INFO: pip is looking at multiple versions of faiss-cpu to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting faiss-cpu\n",
      "  Using cached faiss_cpu-1.9.0.post1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.4 kB)\n",
      "  Using cached faiss_cpu-1.9.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.4 kB)\n",
      "  Using cached faiss_cpu-1.8.0.post1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.7 kB)\n",
      "Collecting packaging (from faiss-cpu)\n",
      "  Using cached packaging-24.2-py3-none-any.whl.metadata (3.2 kB)\n",
      "Collecting python-dateutil>=2.8.2 (from pandas)\n",
      "  Using cached python_dateutil-2.9.0.post0-py2.py3-none-any.whl.metadata (8.4 kB)\n",
      "Collecting pytz>=2020.1 (from pandas)\n",
      "  Using cached pytz-2025.2-py2.py3-none-any.whl.metadata (22 kB)\n",
      "Collecting tzdata>=2022.7 (from pandas)\n",
      "  Using cached tzdata-2025.2-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Collecting scipy>=1.6.0 (from scikit-learn)\n",
      "  Using cached scipy-1.15.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
      "Collecting joblib>=1.2.0 (from scikit-learn)\n",
      "  Using cached joblib-1.4.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting threadpoolctl>=3.1.0 (from scikit-learn)\n",
      "  Using cached threadpoolctl-3.6.0-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting altair<6,>=4.0 (from streamlit)\n",
      "  Using cached altair-5.5.0-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting blinker<2,>=1.0.0 (from streamlit)\n",
      "  Using cached blinker-1.9.0-py3-none-any.whl.metadata (1.6 kB)\n",
      "Collecting cachetools<6,>=4.0 (from streamlit)\n",
      "  Using cached cachetools-5.5.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting click<9,>=7.0 (from streamlit)\n",
      "  Using cached click-8.1.8-py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting pillow<12,>=7.1.0 (from streamlit)\n",
      "  Using cached pillow-11.2.1-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (8.9 kB)\n",
      "Collecting protobuf<6,>=3.20 (from streamlit)\n",
      "  Using cached protobuf-5.29.4-cp38-abi3-manylinux2014_x86_64.whl.metadata (592 bytes)\n",
      "Collecting pyarrow>=7.0 (from streamlit)\n",
      "  Using cached pyarrow-19.0.1-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (3.3 kB)\n",
      "Collecting tenacity<10,>=8.1.0 (from streamlit)\n",
      "  Using cached tenacity-9.1.2-py3-none-any.whl.metadata (1.2 kB)\n",
      "Collecting toml<2,>=0.10.1 (from streamlit)\n",
      "  Using cached toml-0.10.2-py2.py3-none-any.whl.metadata (7.1 kB)\n",
      "Collecting watchdog<7,>=2.1.5 (from streamlit)\n",
      "  Using cached watchdog-6.0.0-py3-none-manylinux2014_x86_64.whl.metadata (44 kB)\n",
      "Collecting gitpython!=3.1.19,<4,>=3.0.7 (from streamlit)\n",
      "  Using cached GitPython-3.1.44-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting pydeck<1,>=0.8.0b4 (from streamlit)\n",
      "  Using cached pydeck-0.9.1-py2.py3-none-any.whl.metadata (4.1 kB)\n",
      "Collecting tornado<7,>=6.0.3 (from streamlit)\n",
      "  Using cached tornado-6.4.2-cp38-abi3-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.5 kB)\n",
      "Collecting aiohttp<4.0.0,>=3.8.3 (from langchain_community)\n",
      "  Using cached aiohttp-3.11.16-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.7 kB)\n",
      "Collecting dataclasses-json<0.7,>=0.5.7 (from langchain_community)\n",
      "  Using cached dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n",
      "Collecting pydantic-settings<3.0.0,>=2.4.0 (from langchain_community)\n",
      "  Using cached pydantic_settings-2.9.1-py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting httpx-sse<1.0.0,>=0.4.0 (from langchain_community)\n",
      "  Using cached httpx_sse-0.4.0-py3-none-any.whl.metadata (9.0 kB)\n",
      "INFO: pip is looking at multiple versions of langchain-community to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting langchain_community\n",
      "  Using cached langchain_community-0.3.20-py3-none-any.whl.metadata (2.4 kB)\n",
      "  Using cached langchain_community-0.3.19-py3-none-any.whl.metadata (2.4 kB)\n",
      "  Using cached langchain_community-0.3.18-py3-none-any.whl.metadata (2.4 kB)\n",
      "  Using cached langchain_community-0.3.17-py3-none-any.whl.metadata (2.4 kB)\n",
      "  Using cached langchain_community-0.3.16-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting ollama<1,>=0.4.4 (from langchain_ollama)\n",
      "  Using cached ollama-0.4.8-py3-none-any.whl.metadata (4.7 kB)\n",
      "Collecting regex>=2022.1.18 (from tiktoken)\n",
      "  Using cached regex-2024.11.6-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (40 kB)\n",
      "Collecting aiohappyeyeballs>=2.3.0 (from aiohttp<4.0.0,>=3.8.3->langchain_community)\n",
      "  Using cached aiohappyeyeballs-2.6.1-py3-none-any.whl.metadata (5.9 kB)\n",
      "Collecting aiosignal>=1.1.2 (from aiohttp<4.0.0,>=3.8.3->langchain_community)\n",
      "  Using cached aiosignal-1.3.2-py2.py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting attrs>=17.3.0 (from aiohttp<4.0.0,>=3.8.3->langchain_community)\n",
      "  Using cached attrs-25.3.0-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting frozenlist>=1.1.1 (from aiohttp<4.0.0,>=3.8.3->langchain_community)\n",
      "  Using cached frozenlist-1.6.0-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (16 kB)\n",
      "Collecting multidict<7.0,>=4.5 (from aiohttp<4.0.0,>=3.8.3->langchain_community)\n",
      "  Using cached multidict-6.4.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.3 kB)\n",
      "Collecting propcache>=0.2.0 (from aiohttp<4.0.0,>=3.8.3->langchain_community)\n",
      "  Using cached propcache-0.3.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (10 kB)\n",
      "Collecting yarl<2.0,>=1.17.0 (from aiohttp<4.0.0,>=3.8.3->langchain_community)\n",
      "  Using cached yarl-1.20.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (72 kB)\n",
      "Collecting jinja2 (from altair<6,>=4.0->streamlit)\n",
      "  Using cached jinja2-3.1.6-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting jsonschema>=3.0 (from altair<6,>=4.0->streamlit)\n",
      "  Using cached jsonschema-4.23.0-py3-none-any.whl.metadata (7.9 kB)\n",
      "Collecting narwhals>=1.14.2 (from altair<6,>=4.0->streamlit)\n",
      "  Using cached narwhals-1.35.0-py3-none-any.whl.metadata (9.2 kB)\n",
      "Collecting idna>=2.8 (from anyio<5,>=3.5.0->openai)\n",
      "  Using cached idna-3.10-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain_community)\n",
      "  Using cached marshmallow-3.26.1-py3-none-any.whl.metadata (7.3 kB)\n",
      "Collecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain_community)\n",
      "  Using cached typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
      "Collecting gitdb<5,>=4.0.1 (from gitpython!=3.1.19,<4,>=3.0.7->streamlit)\n",
      "  Using cached gitdb-4.0.12-py3-none-any.whl.metadata (1.2 kB)\n",
      "Collecting certifi (from httpx<1,>=0.23.0->openai)\n",
      "  Using cached certifi-2025.1.31-py3-none-any.whl.metadata (2.5 kB)\n",
      "Collecting httpcore==1.* (from httpx<1,>=0.23.0->openai)\n",
      "  Using cached httpcore-1.0.8-py3-none-any.whl.metadata (21 kB)\n",
      "Collecting h11<0.15,>=0.13 (from httpcore==1.*->httpx<1,>=0.23.0->openai)\n",
      "  Using cached h11-0.14.0-py3-none-any.whl.metadata (8.2 kB)\n",
      "Collecting jsonpatch<2.0,>=1.33 (from langchain-core<1.0.0,>=0.3.51->langchain)\n",
      "  Using cached jsonpatch-1.33-py2.py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting orjson<4.0.0,>=3.9.14 (from langsmith<0.4,>=0.1.17->langchain)\n",
      "  Using cached orjson-3.10.16-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (41 kB)\n",
      "Collecting requests-toolbelt<2.0.0,>=1.0.0 (from langsmith<0.4,>=0.1.17->langchain)\n",
      "  Using cached requests_toolbelt-1.0.0-py2.py3-none-any.whl.metadata (14 kB)\n",
      "Collecting zstandard<0.24.0,>=0.23.0 (from langsmith<0.4,>=0.1.17->langchain)\n",
      "  Using cached zstandard-0.23.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.0 kB)\n",
      "Collecting annotated-types>=0.6.0 (from pydantic<3.0.0,>=2.7.4->langchain)\n",
      "  Using cached annotated_types-0.7.0-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting pydantic-core==2.33.1 (from pydantic<3.0.0,>=2.7.4->langchain)\n",
      "  Using cached pydantic_core-2.33.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.8 kB)\n",
      "Collecting typing-inspection>=0.4.0 (from pydantic<3.0.0,>=2.7.4->langchain)\n",
      "  Using cached typing_inspection-0.4.0-py3-none-any.whl.metadata (2.6 kB)\n",
      "Collecting python-dotenv>=0.21.0 (from pydantic-settings<3.0.0,>=2.4.0->langchain_community)\n",
      "  Using cached python_dotenv-1.1.0-py3-none-any.whl.metadata (24 kB)\n",
      "Collecting six>=1.5 (from python-dateutil>=2.8.2->pandas)\n",
      "  Using cached six-1.17.0-py2.py3-none-any.whl.metadata (1.7 kB)\n",
      "Collecting charset-normalizer<4,>=2 (from requests<3,>=2->langchain)\n",
      "  Using cached charset_normalizer-3.4.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (35 kB)\n",
      "Collecting urllib3<3,>=1.21.1 (from requests<3,>=2->langchain)\n",
      "  Using cached urllib3-2.4.0-py3-none-any.whl.metadata (6.5 kB)\n",
      "Collecting greenlet>=1 (from SQLAlchemy<3,>=1.4->langchain)\n",
      "  Using cached greenlet-3.2.0-cp311-cp311-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl.metadata (4.1 kB)\n",
      "Collecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->gitpython!=3.1.19,<4,>=3.0.7->streamlit)\n",
      "  Using cached smmap-5.0.2-py3-none-any.whl.metadata (4.3 kB)\n",
      "Collecting MarkupSafe>=2.0 (from jinja2->altair<6,>=4.0->streamlit)\n",
      "  Using cached MarkupSafe-3.0.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.0 kB)\n",
      "Collecting jsonpointer>=1.9 (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.51->langchain)\n",
      "  Using cached jsonpointer-3.0.0-py2.py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting jsonschema-specifications>=2023.03.6 (from jsonschema>=3.0->altair<6,>=4.0->streamlit)\n",
      "  Using cached jsonschema_specifications-2024.10.1-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting referencing>=0.28.4 (from jsonschema>=3.0->altair<6,>=4.0->streamlit)\n",
      "  Using cached referencing-0.36.2-py3-none-any.whl.metadata (2.8 kB)\n",
      "Collecting rpds-py>=0.7.1 (from jsonschema>=3.0->altair<6,>=4.0->streamlit)\n",
      "  Using cached rpds_py-0.24.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.1 kB)\n",
      "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain_community)\n",
      "  Using cached mypy_extensions-1.0.0-py3-none-any.whl.metadata (1.1 kB)\n",
      "Using cached numpy-1.24.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.3 MB)\n",
      "Using cached langchain-0.3.23-py3-none-any.whl (1.0 MB)\n",
      "Using cached openai-1.75.0-py3-none-any.whl (646 kB)\n",
      "Using cached faiss_cpu-1.8.0.post1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (27.0 MB)\n",
      "Using cached pandas-2.2.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.1 MB)\n",
      "Using cached scikit_learn-1.6.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.5 MB)\n",
      "Using cached streamlit-1.44.1-py3-none-any.whl (9.8 MB)\n",
      "Using cached langchain_community-0.3.16-py3-none-any.whl (2.5 MB)\n",
      "Using cached langchain_ollama-0.3.2-py3-none-any.whl (20 kB)\n",
      "Using cached tiktoken-0.9.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
      "Using cached langchain_openai-0.3.14-py3-none-any.whl (62 kB)\n",
      "Using cached aiohttp-3.11.16-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.7 MB)\n",
      "Using cached altair-5.5.0-py3-none-any.whl (731 kB)\n",
      "Using cached anyio-4.9.0-py3-none-any.whl (100 kB)\n",
      "Using cached blinker-1.9.0-py3-none-any.whl (8.5 kB)\n",
      "Using cached cachetools-5.5.2-py3-none-any.whl (10 kB)\n",
      "Using cached click-8.1.8-py3-none-any.whl (98 kB)\n",
      "Using cached dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
      "Using cached distro-1.9.0-py3-none-any.whl (20 kB)\n",
      "Using cached GitPython-3.1.44-py3-none-any.whl (207 kB)\n",
      "Using cached httpx-0.28.1-py3-none-any.whl (73 kB)\n",
      "Using cached httpcore-1.0.8-py3-none-any.whl (78 kB)\n",
      "Using cached httpx_sse-0.4.0-py3-none-any.whl (7.8 kB)\n",
      "Using cached jiter-0.9.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (351 kB)\n",
      "Using cached joblib-1.4.2-py3-none-any.whl (301 kB)\n",
      "Using cached langchain_core-0.3.54-py3-none-any.whl (433 kB)\n",
      "Using cached langchain_text_splitters-0.3.8-py3-none-any.whl (32 kB)\n",
      "Using cached langsmith-0.3.32-py3-none-any.whl (358 kB)\n",
      "Using cached ollama-0.4.8-py3-none-any.whl (13 kB)\n",
      "Using cached packaging-24.2-py3-none-any.whl (65 kB)\n",
      "Using cached pillow-11.2.1-cp311-cp311-manylinux_2_28_x86_64.whl (4.6 MB)\n",
      "Using cached protobuf-5.29.4-cp38-abi3-manylinux2014_x86_64.whl (319 kB)\n",
      "Using cached pyarrow-19.0.1-cp311-cp311-manylinux_2_28_x86_64.whl (42.1 MB)\n",
      "Using cached pydantic-2.11.3-py3-none-any.whl (443 kB)\n",
      "Using cached pydantic_core-2.33.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.0 MB)\n",
      "Using cached pydantic_settings-2.9.1-py3-none-any.whl (44 kB)\n",
      "Using cached pydeck-0.9.1-py2.py3-none-any.whl (6.9 MB)\n",
      "Using cached python_dateutil-2.9.0.post0-py2.py3-none-any.whl (229 kB)\n",
      "Using cached pytz-2025.2-py2.py3-none-any.whl (509 kB)\n",
      "Using cached PyYAML-6.0.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (762 kB)\n",
      "Using cached regex-2024.11.6-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (792 kB)\n",
      "Using cached requests-2.32.3-py3-none-any.whl (64 kB)\n",
      "Using cached scipy-1.15.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (37.6 MB)\n",
      "Using cached sniffio-1.3.1-py3-none-any.whl (10 kB)\n",
      "Using cached sqlalchemy-2.0.40-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.2 MB)\n",
      "Using cached tenacity-9.1.2-py3-none-any.whl (28 kB)\n",
      "Using cached threadpoolctl-3.6.0-py3-none-any.whl (18 kB)\n",
      "Using cached toml-0.10.2-py2.py3-none-any.whl (16 kB)\n",
      "Using cached tornado-6.4.2-cp38-abi3-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (437 kB)\n",
      "Using cached tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "Using cached typing_extensions-4.13.2-py3-none-any.whl (45 kB)\n",
      "Using cached tzdata-2025.2-py2.py3-none-any.whl (347 kB)\n",
      "Using cached watchdog-6.0.0-py3-none-manylinux2014_x86_64.whl (79 kB)\n",
      "Using cached aiohappyeyeballs-2.6.1-py3-none-any.whl (15 kB)\n",
      "Using cached aiosignal-1.3.2-py2.py3-none-any.whl (7.6 kB)\n",
      "Using cached annotated_types-0.7.0-py3-none-any.whl (13 kB)\n",
      "Using cached attrs-25.3.0-py3-none-any.whl (63 kB)\n",
      "Using cached certifi-2025.1.31-py3-none-any.whl (166 kB)\n",
      "Using cached charset_normalizer-3.4.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (143 kB)\n",
      "Using cached frozenlist-1.6.0-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (313 kB)\n",
      "Using cached gitdb-4.0.12-py3-none-any.whl (62 kB)\n",
      "Using cached greenlet-3.2.0-cp311-cp311-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl (583 kB)\n",
      "Using cached idna-3.10-py3-none-any.whl (70 kB)\n",
      "Using cached jinja2-3.1.6-py3-none-any.whl (134 kB)\n",
      "Using cached jsonpatch-1.33-py2.py3-none-any.whl (12 kB)\n",
      "Using cached jsonschema-4.23.0-py3-none-any.whl (88 kB)\n",
      "Using cached marshmallow-3.26.1-py3-none-any.whl (50 kB)\n",
      "Using cached multidict-6.4.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (223 kB)\n",
      "Using cached narwhals-1.35.0-py3-none-any.whl (325 kB)\n",
      "Using cached orjson-3.10.16-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (132 kB)\n",
      "Using cached propcache-0.3.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (232 kB)\n",
      "Using cached python_dotenv-1.1.0-py3-none-any.whl (20 kB)\n",
      "Using cached requests_toolbelt-1.0.0-py2.py3-none-any.whl (54 kB)\n",
      "Using cached six-1.17.0-py2.py3-none-any.whl (11 kB)\n",
      "Using cached typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
      "Using cached typing_inspection-0.4.0-py3-none-any.whl (14 kB)\n",
      "Using cached urllib3-2.4.0-py3-none-any.whl (128 kB)\n",
      "Using cached yarl-1.20.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (358 kB)\n",
      "Using cached zstandard-0.23.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.4 MB)\n",
      "Using cached h11-0.14.0-py3-none-any.whl (58 kB)\n",
      "Using cached jsonpointer-3.0.0-py2.py3-none-any.whl (7.6 kB)\n",
      "Using cached jsonschema_specifications-2024.10.1-py3-none-any.whl (18 kB)\n",
      "Using cached MarkupSafe-3.0.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (23 kB)\n",
      "Using cached mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
      "Using cached referencing-0.36.2-py3-none-any.whl (26 kB)\n",
      "Using cached rpds_py-0.24.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (389 kB)\n",
      "Using cached smmap-5.0.2-py3-none-any.whl (24 kB)\n",
      "Installing collected packages: pytz, zstandard, watchdog, urllib3, tzdata, typing-extensions, tqdm, tornado, toml, threadpoolctl, tenacity, sniffio, smmap, six, rpds-py, regex, PyYAML, python-dotenv, pyarrow, protobuf, propcache, pillow, packaging, orjson, numpy, narwhals, mypy-extensions, multidict, MarkupSafe, jsonpointer, joblib, jiter, idna, httpx-sse, h11, greenlet, frozenlist, distro, click, charset-normalizer, certifi, cachetools, blinker, attrs, annotated-types, aiohappyeyeballs, yarl, typing-inspection, typing-inspect, SQLAlchemy, scipy, requests, referencing, python-dateutil, pydantic-core, marshmallow, jsonpatch, jinja2, httpcore, gitdb, faiss-cpu, anyio, aiosignal, tiktoken, scikit-learn, requests-toolbelt, pydeck, pydantic, pandas, jsonschema-specifications, httpx, gitpython, dataclasses-json, aiohttp, pydantic-settings, openai, ollama, langsmith, jsonschema, langchain-core, altair, streamlit, langchain-text-splitters, langchain-openai, langchain_ollama, langchain, langchain_community\n",
      "  Attempting uninstall: pytz\n",
      "    Found existing installation: pytz 2025.2\n",
      "    Uninstalling pytz-2025.2:\n",
      "      Successfully uninstalled pytz-2025.2\n",
      "  Attempting uninstall: zstandard\n",
      "    Found existing installation: zstandard 0.23.0\n",
      "    Uninstalling zstandard-0.23.0:\n",
      "      Successfully uninstalled zstandard-0.23.0\n",
      "  Attempting uninstall: watchdog\n",
      "    Found existing installation: watchdog 6.0.0\n",
      "    Uninstalling watchdog-6.0.0:\n",
      "      Successfully uninstalled watchdog-6.0.0\n",
      "  Attempting uninstall: urllib3\n",
      "    Found existing installation: urllib3 2.4.0\n",
      "    Uninstalling urllib3-2.4.0:\n",
      "      Successfully uninstalled urllib3-2.4.0\n",
      "  Attempting uninstall: tzdata\n",
      "    Found existing installation: tzdata 2025.2\n",
      "    Uninstalling tzdata-2025.2:\n",
      "      Successfully uninstalled tzdata-2025.2\n",
      "  Attempting uninstall: typing-extensions\n",
      "    Found existing installation: typing_extensions 4.13.2\n",
      "    Uninstalling typing_extensions-4.13.2:\n",
      "      Successfully uninstalled typing_extensions-4.13.2\n",
      "  Attempting uninstall: tqdm\n",
      "    Found existing installation: tqdm 4.67.1\n",
      "    Uninstalling tqdm-4.67.1:\n",
      "      Successfully uninstalled tqdm-4.67.1\n",
      "  Attempting uninstall: tornado\n",
      "    Found existing installation: tornado 6.4.2\n",
      "    Uninstalling tornado-6.4.2:\n",
      "      Successfully uninstalled tornado-6.4.2\n",
      "  Attempting uninstall: toml\n",
      "    Found existing installation: toml 0.10.2\n",
      "    Uninstalling toml-0.10.2:\n",
      "      Successfully uninstalled toml-0.10.2\n",
      "  Attempting uninstall: threadpoolctl\n",
      "    Found existing installation: threadpoolctl 3.6.0\n",
      "    Uninstalling threadpoolctl-3.6.0:\n",
      "      Successfully uninstalled threadpoolctl-3.6.0\n",
      "  Attempting uninstall: tenacity\n",
      "    Found existing installation: tenacity 9.1.2\n",
      "    Uninstalling tenacity-9.1.2:\n",
      "      Successfully uninstalled tenacity-9.1.2\n",
      "  Attempting uninstall: sniffio\n",
      "    Found existing installation: sniffio 1.3.1\n",
      "    Uninstalling sniffio-1.3.1:\n",
      "      Successfully uninstalled sniffio-1.3.1\n",
      "  Attempting uninstall: smmap\n",
      "    Found existing installation: smmap 5.0.2\n",
      "    Uninstalling smmap-5.0.2:\n",
      "      Successfully uninstalled smmap-5.0.2\n",
      "  Attempting uninstall: six\n",
      "    Found existing installation: six 1.17.0\n",
      "    Uninstalling six-1.17.0:\n",
      "      Successfully uninstalled six-1.17.0\n",
      "  Attempting uninstall: rpds-py\n",
      "    Found existing installation: rpds-py 0.24.0\n",
      "    Uninstalling rpds-py-0.24.0:\n",
      "      Successfully uninstalled rpds-py-0.24.0\n",
      "  Attempting uninstall: regex\n",
      "    Found existing installation: regex 2024.11.6\n",
      "    Uninstalling regex-2024.11.6:\n",
      "      Successfully uninstalled regex-2024.11.6\n",
      "  Attempting uninstall: PyYAML\n",
      "    Found existing installation: PyYAML 6.0.2\n",
      "    Uninstalling PyYAML-6.0.2:\n",
      "      Successfully uninstalled PyYAML-6.0.2\n",
      "  Attempting uninstall: python-dotenv\n",
      "    Found existing installation: python-dotenv 1.1.0\n",
      "    Uninstalling python-dotenv-1.1.0:\n",
      "      Successfully uninstalled python-dotenv-1.1.0\n",
      "  Attempting uninstall: pyarrow\n",
      "    Found existing installation: pyarrow 19.0.1\n",
      "    Uninstalling pyarrow-19.0.1:\n",
      "      Successfully uninstalled pyarrow-19.0.1\n",
      "  Attempting uninstall: protobuf\n",
      "    Found existing installation: protobuf 5.29.4\n",
      "    Uninstalling protobuf-5.29.4:\n",
      "      Successfully uninstalled protobuf-5.29.4\n",
      "  Attempting uninstall: propcache\n",
      "    Found existing installation: propcache 0.3.1\n",
      "    Uninstalling propcache-0.3.1:\n",
      "      Successfully uninstalled propcache-0.3.1\n",
      "  Attempting uninstall: pillow\n",
      "    Found existing installation: pillow 11.2.1\n",
      "    Uninstalling pillow-11.2.1:\n",
      "      Successfully uninstalled pillow-11.2.1\n",
      "  Attempting uninstall: packaging\n",
      "    Found existing installation: packaging 24.2\n",
      "    Uninstalling packaging-24.2:\n",
      "      Successfully uninstalled packaging-24.2\n",
      "  Attempting uninstall: orjson\n",
      "    Found existing installation: orjson 3.10.16\n",
      "    Uninstalling orjson-3.10.16:\n",
      "      Successfully uninstalled orjson-3.10.16\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 1.24.3\n",
      "    Uninstalling numpy-1.24.3:\n",
      "      Successfully uninstalled numpy-1.24.3\n",
      "  Attempting uninstall: narwhals\n",
      "    Found existing installation: narwhals 1.35.0\n",
      "    Uninstalling narwhals-1.35.0:\n",
      "      Successfully uninstalled narwhals-1.35.0\n",
      "  Attempting uninstall: mypy-extensions\n",
      "    Found existing installation: mypy-extensions 1.0.0\n",
      "    Uninstalling mypy-extensions-1.0.0:\n",
      "      Successfully uninstalled mypy-extensions-1.0.0\n",
      "  Attempting uninstall: multidict\n",
      "    Found existing installation: multidict 6.4.3\n",
      "    Uninstalling multidict-6.4.3:\n",
      "      Successfully uninstalled multidict-6.4.3\n",
      "  Attempting uninstall: MarkupSafe\n",
      "    Found existing installation: MarkupSafe 3.0.2\n",
      "    Uninstalling MarkupSafe-3.0.2:\n",
      "      Successfully uninstalled MarkupSafe-3.0.2\n",
      "  Attempting uninstall: jsonpointer\n",
      "    Found existing installation: jsonpointer 3.0.0\n",
      "    Uninstalling jsonpointer-3.0.0:\n",
      "      Successfully uninstalled jsonpointer-3.0.0\n",
      "  Attempting uninstall: joblib\n",
      "    Found existing installation: joblib 1.4.2\n",
      "    Uninstalling joblib-1.4.2:\n",
      "      Successfully uninstalled joblib-1.4.2\n",
      "  Attempting uninstall: jiter\n",
      "    Found existing installation: jiter 0.9.0\n",
      "    Uninstalling jiter-0.9.0:\n",
      "      Successfully uninstalled jiter-0.9.0\n",
      "  Attempting uninstall: idna\n",
      "    Found existing installation: idna 3.10\n",
      "    Uninstalling idna-3.10:\n",
      "      Successfully uninstalled idna-3.10\n",
      "  Attempting uninstall: httpx-sse\n",
      "    Found existing installation: httpx-sse 0.4.0\n",
      "    Uninstalling httpx-sse-0.4.0:\n",
      "      Successfully uninstalled httpx-sse-0.4.0\n",
      "  Attempting uninstall: h11\n",
      "    Found existing installation: h11 0.14.0\n",
      "    Uninstalling h11-0.14.0:\n",
      "      Successfully uninstalled h11-0.14.0\n",
      "  Attempting uninstall: greenlet\n",
      "    Found existing installation: greenlet 3.2.0\n",
      "    Uninstalling greenlet-3.2.0:\n",
      "      Successfully uninstalled greenlet-3.2.0\n",
      "  Attempting uninstall: frozenlist\n",
      "    Found existing installation: frozenlist 1.6.0\n",
      "    Uninstalling frozenlist-1.6.0:\n",
      "      Successfully uninstalled frozenlist-1.6.0\n",
      "  Attempting uninstall: distro\n",
      "    Found existing installation: distro 1.9.0\n",
      "    Uninstalling distro-1.9.0:\n",
      "      Successfully uninstalled distro-1.9.0\n",
      "  Attempting uninstall: click\n",
      "    Found existing installation: click 8.1.8\n",
      "    Uninstalling click-8.1.8:\n",
      "      Successfully uninstalled click-8.1.8\n",
      "  Attempting uninstall: charset-normalizer\n",
      "    Found existing installation: charset-normalizer 3.4.1\n",
      "    Uninstalling charset-normalizer-3.4.1:\n",
      "      Successfully uninstalled charset-normalizer-3.4.1\n",
      "  Attempting uninstall: certifi\n",
      "    Found existing installation: certifi 2025.1.31\n",
      "    Uninstalling certifi-2025.1.31:\n",
      "      Successfully uninstalled certifi-2025.1.31\n",
      "  Attempting uninstall: cachetools\n",
      "    Found existing installation: cachetools 5.5.2\n",
      "    Uninstalling cachetools-5.5.2:\n",
      "      Successfully uninstalled cachetools-5.5.2\n",
      "  Attempting uninstall: blinker\n",
      "    Found existing installation: blinker 1.9.0\n",
      "    Uninstalling blinker-1.9.0:\n",
      "      Successfully uninstalled blinker-1.9.0\n",
      "  Attempting uninstall: attrs\n",
      "    Found existing installation: attrs 25.3.0\n",
      "    Uninstalling attrs-25.3.0:\n",
      "      Successfully uninstalled attrs-25.3.0\n",
      "  Attempting uninstall: annotated-types\n",
      "    Found existing installation: annotated-types 0.7.0\n",
      "    Uninstalling annotated-types-0.7.0:\n",
      "      Successfully uninstalled annotated-types-0.7.0\n",
      "  Attempting uninstall: aiohappyeyeballs\n",
      "    Found existing installation: aiohappyeyeballs 2.6.1\n",
      "    Uninstalling aiohappyeyeballs-2.6.1:\n",
      "      Successfully uninstalled aiohappyeyeballs-2.6.1\n",
      "  Attempting uninstall: yarl\n",
      "    Found existing installation: yarl 1.20.0\n",
      "    Uninstalling yarl-1.20.0:\n",
      "      Successfully uninstalled yarl-1.20.0\n",
      "  Attempting uninstall: typing-inspection\n",
      "    Found existing installation: typing-inspection 0.4.0\n",
      "    Uninstalling typing-inspection-0.4.0:\n",
      "      Successfully uninstalled typing-inspection-0.4.0\n",
      "  Attempting uninstall: typing-inspect\n",
      "    Found existing installation: typing-inspect 0.9.0\n",
      "    Uninstalling typing-inspect-0.9.0:\n",
      "      Successfully uninstalled typing-inspect-0.9.0\n",
      "  Attempting uninstall: SQLAlchemy\n",
      "    Found existing installation: SQLAlchemy 2.0.40\n",
      "    Uninstalling SQLAlchemy-2.0.40:\n",
      "      Successfully uninstalled SQLAlchemy-2.0.40\n",
      "  Attempting uninstall: scipy\n",
      "    Found existing installation: scipy 1.15.2\n",
      "    Uninstalling scipy-1.15.2:\n",
      "      Successfully uninstalled scipy-1.15.2\n",
      "  Attempting uninstall: requests\n",
      "    Found existing installation: requests 2.32.3\n",
      "    Uninstalling requests-2.32.3:\n",
      "      Successfully uninstalled requests-2.32.3\n",
      "  Attempting uninstall: referencing\n",
      "    Found existing installation: referencing 0.36.2\n",
      "    Uninstalling referencing-0.36.2:\n",
      "      Successfully uninstalled referencing-0.36.2\n",
      "  Attempting uninstall: python-dateutil\n",
      "    Found existing installation: python-dateutil 2.9.0.post0\n",
      "    Uninstalling python-dateutil-2.9.0.post0:\n",
      "      Successfully uninstalled python-dateutil-2.9.0.post0\n",
      "  Attempting uninstall: pydantic-core\n",
      "    Found existing installation: pydantic_core 2.33.1\n",
      "    Uninstalling pydantic_core-2.33.1:\n",
      "      Successfully uninstalled pydantic_core-2.33.1\n",
      "  Attempting uninstall: marshmallow\n",
      "    Found existing installation: marshmallow 3.26.1\n",
      "    Uninstalling marshmallow-3.26.1:\n",
      "      Successfully uninstalled marshmallow-3.26.1\n",
      "  Attempting uninstall: jsonpatch\n",
      "    Found existing installation: jsonpatch 1.33\n",
      "    Uninstalling jsonpatch-1.33:\n",
      "      Successfully uninstalled jsonpatch-1.33\n",
      "  Attempting uninstall: jinja2\n",
      "    Found existing installation: Jinja2 3.1.6\n",
      "    Uninstalling Jinja2-3.1.6:\n",
      "      Successfully uninstalled Jinja2-3.1.6\n",
      "  Attempting uninstall: httpcore\n",
      "    Found existing installation: httpcore 1.0.8\n",
      "    Uninstalling httpcore-1.0.8:\n",
      "      Successfully uninstalled httpcore-1.0.8\n",
      "  Attempting uninstall: gitdb\n",
      "    Found existing installation: gitdb 4.0.12\n",
      "    Uninstalling gitdb-4.0.12:\n",
      "      Successfully uninstalled gitdb-4.0.12\n",
      "  Attempting uninstall: faiss-cpu\n",
      "    Found existing installation: faiss-cpu 1.8.0.post1\n",
      "    Uninstalling faiss-cpu-1.8.0.post1:\n",
      "      Successfully uninstalled faiss-cpu-1.8.0.post1\n",
      "  Attempting uninstall: anyio\n",
      "    Found existing installation: anyio 4.9.0\n",
      "    Uninstalling anyio-4.9.0:\n",
      "      Successfully uninstalled anyio-4.9.0\n",
      "  Attempting uninstall: aiosignal\n",
      "    Found existing installation: aiosignal 1.3.2\n",
      "    Uninstalling aiosignal-1.3.2:\n",
      "      Successfully uninstalled aiosignal-1.3.2\n",
      "  Attempting uninstall: tiktoken\n",
      "    Found existing installation: tiktoken 0.9.0\n",
      "    Uninstalling tiktoken-0.9.0:\n",
      "      Successfully uninstalled tiktoken-0.9.0\n",
      "  Attempting uninstall: scikit-learn\n",
      "    Found existing installation: scikit-learn 1.6.1\n",
      "    Uninstalling scikit-learn-1.6.1:\n",
      "      Successfully uninstalled scikit-learn-1.6.1\n",
      "  Attempting uninstall: requests-toolbelt\n",
      "    Found existing installation: requests-toolbelt 1.0.0\n",
      "    Uninstalling requests-toolbelt-1.0.0:\n",
      "      Successfully uninstalled requests-toolbelt-1.0.0\n",
      "  Attempting uninstall: pydeck\n",
      "    Found existing installation: pydeck 0.9.1\n",
      "    Uninstalling pydeck-0.9.1:\n",
      "      Successfully uninstalled pydeck-0.9.1\n",
      "  Attempting uninstall: pydantic\n",
      "    Found existing installation: pydantic 2.11.3\n",
      "    Uninstalling pydantic-2.11.3:\n",
      "      Successfully uninstalled pydantic-2.11.3\n",
      "  Attempting uninstall: pandas\n",
      "    Found existing installation: pandas 2.2.3\n",
      "    Uninstalling pandas-2.2.3:\n",
      "      Successfully uninstalled pandas-2.2.3\n",
      "  Attempting uninstall: jsonschema-specifications\n",
      "    Found existing installation: jsonschema-specifications 2024.10.1\n",
      "    Uninstalling jsonschema-specifications-2024.10.1:\n",
      "      Successfully uninstalled jsonschema-specifications-2024.10.1\n",
      "  Attempting uninstall: httpx\n",
      "    Found existing installation: httpx 0.28.1\n",
      "    Uninstalling httpx-0.28.1:\n",
      "      Successfully uninstalled httpx-0.28.1\n",
      "  Attempting uninstall: gitpython\n",
      "    Found existing installation: GitPython 3.1.44\n",
      "    Uninstalling GitPython-3.1.44:\n",
      "      Successfully uninstalled GitPython-3.1.44\n",
      "  Attempting uninstall: dataclasses-json\n",
      "    Found existing installation: dataclasses-json 0.6.7\n",
      "    Uninstalling dataclasses-json-0.6.7:\n",
      "      Successfully uninstalled dataclasses-json-0.6.7\n",
      "  Attempting uninstall: aiohttp\n",
      "    Found existing installation: aiohttp 3.11.16\n",
      "    Uninstalling aiohttp-3.11.16:\n",
      "      Successfully uninstalled aiohttp-3.11.16\n",
      "  Attempting uninstall: pydantic-settings\n",
      "    Found existing installation: pydantic-settings 2.9.1\n",
      "    Uninstalling pydantic-settings-2.9.1:\n",
      "      Successfully uninstalled pydantic-settings-2.9.1\n",
      "  Attempting uninstall: openai\n",
      "    Found existing installation: openai 1.75.0\n",
      "    Uninstalling openai-1.75.0:\n",
      "      Successfully uninstalled openai-1.75.0\n",
      "  Attempting uninstall: ollama\n",
      "    Found existing installation: ollama 0.4.8\n",
      "    Uninstalling ollama-0.4.8:\n",
      "      Successfully uninstalled ollama-0.4.8\n",
      "  Attempting uninstall: langsmith\n",
      "    Found existing installation: langsmith 0.3.32\n",
      "    Uninstalling langsmith-0.3.32:\n",
      "      Successfully uninstalled langsmith-0.3.32\n",
      "  Attempting uninstall: jsonschema\n",
      "    Found existing installation: jsonschema 4.23.0\n",
      "    Uninstalling jsonschema-4.23.0:\n",
      "      Successfully uninstalled jsonschema-4.23.0\n",
      "  Attempting uninstall: langchain-core\n",
      "    Found existing installation: langchain-core 0.3.54\n",
      "    Uninstalling langchain-core-0.3.54:\n",
      "      Successfully uninstalled langchain-core-0.3.54\n",
      "  Attempting uninstall: altair\n",
      "    Found existing installation: altair 5.5.0\n",
      "    Uninstalling altair-5.5.0:\n",
      "      Successfully uninstalled altair-5.5.0\n",
      "  Attempting uninstall: streamlit\n",
      "    Found existing installation: streamlit 1.44.1\n",
      "    Uninstalling streamlit-1.44.1:\n",
      "      Successfully uninstalled streamlit-1.44.1\n",
      "  Attempting uninstall: langchain-text-splitters\n",
      "    Found existing installation: langchain-text-splitters 0.3.8\n",
      "    Uninstalling langchain-text-splitters-0.3.8:\n",
      "      Successfully uninstalled langchain-text-splitters-0.3.8\n",
      "  Attempting uninstall: langchain-openai\n",
      "    Found existing installation: langchain-openai 0.3.14\n",
      "    Uninstalling langchain-openai-0.3.14:\n",
      "      Successfully uninstalled langchain-openai-0.3.14\n",
      "  Attempting uninstall: langchain_ollama\n",
      "    Found existing installation: langchain-ollama 0.3.2\n",
      "    Uninstalling langchain-ollama-0.3.2:\n",
      "      Successfully uninstalled langchain-ollama-0.3.2\n",
      "  Attempting uninstall: langchain\n",
      "    Found existing installation: langchain 0.3.23\n",
      "    Uninstalling langchain-0.3.23:\n",
      "      Successfully uninstalled langchain-0.3.23\n",
      "  Attempting uninstall: langchain_community\n",
      "    Found existing installation: langchain-community 0.3.16\n",
      "    Uninstalling langchain-community-0.3.16:\n",
      "      Successfully uninstalled langchain-community-0.3.16\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "google-colab 1.0.0 requires pandas==2.2.2, but you have pandas 2.2.3 which is incompatible.\n",
      "thinc 8.3.6 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.24.3 which is incompatible.\n",
      "torch 2.6.0+cu124 requires nvidia-cublas-cu12==12.4.5.8; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cublas-cu12 12.5.3.2 which is incompatible.\n",
      "torch 2.6.0+cu124 requires nvidia-cuda-cupti-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-cupti-cu12 12.5.82 which is incompatible.\n",
      "torch 2.6.0+cu124 requires nvidia-cuda-nvrtc-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-nvrtc-cu12 12.5.82 which is incompatible.\n",
      "torch 2.6.0+cu124 requires nvidia-cuda-runtime-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-runtime-cu12 12.5.82 which is incompatible.\n",
      "torch 2.6.0+cu124 requires nvidia-cudnn-cu12==9.1.0.70; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cudnn-cu12 9.3.0.75 which is incompatible.\n",
      "torch 2.6.0+cu124 requires nvidia-cufft-cu12==11.2.1.3; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cufft-cu12 11.2.3.61 which is incompatible.\n",
      "torch 2.6.0+cu124 requires nvidia-curand-cu12==10.3.5.147; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-curand-cu12 10.3.6.82 which is incompatible.\n",
      "torch 2.6.0+cu124 requires nvidia-cusolver-cu12==11.6.1.9; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusolver-cu12 11.6.3.83 which is incompatible.\n",
      "torch 2.6.0+cu124 requires nvidia-cusparse-cu12==12.3.1.170; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusparse-cu12 12.5.1.3 which is incompatible.\n",
      "torch 2.6.0+cu124 requires nvidia-nvjitlink-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-nvjitlink-cu12 12.5.82 which is incompatible.\n",
      "albumentations 2.0.5 requires numpy>=1.24.4, but you have numpy 1.24.3 which is incompatible.\n",
      "jax 0.5.2 requires numpy>=1.25, but you have numpy 1.24.3 which is incompatible.\n",
      "albucore 0.0.23 requires numpy>=1.24.4, but you have numpy 1.24.3 which is incompatible.\n",
      "blosc2 3.3.0 requires numpy>=1.26, but you have numpy 1.24.3 which is incompatible.\n",
      "jaxlib 0.5.1 requires numpy>=1.25, but you have numpy 1.24.3 which is incompatible.\n",
      "treescope 0.1.9 requires numpy>=1.25.2, but you have numpy 1.24.3 which is incompatible.\n",
      "pymc 5.21.2 requires numpy>=1.25.0, but you have numpy 1.24.3 which is incompatible.\n",
      "tensorflow 2.18.0 requires numpy<2.1.0,>=1.26.0, but you have numpy 1.24.3 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed MarkupSafe-3.0.2 PyYAML-6.0.2 SQLAlchemy-2.0.40 aiohappyeyeballs-2.6.1 aiohttp-3.11.16 aiosignal-1.3.2 altair-5.5.0 annotated-types-0.7.0 anyio-4.9.0 attrs-25.3.0 blinker-1.9.0 cachetools-5.5.2 certifi-2025.1.31 charset-normalizer-3.4.1 click-8.1.8 dataclasses-json-0.6.7 distro-1.9.0 faiss-cpu-1.8.0.post1 frozenlist-1.6.0 gitdb-4.0.12 gitpython-3.1.44 greenlet-3.2.0 h11-0.14.0 httpcore-1.0.8 httpx-0.28.1 httpx-sse-0.4.0 idna-3.10 jinja2-3.1.6 jiter-0.9.0 joblib-1.4.2 jsonpatch-1.33 jsonpointer-3.0.0 jsonschema-4.23.0 jsonschema-specifications-2024.10.1 langchain-0.3.23 langchain-core-0.3.54 langchain-openai-0.3.14 langchain-text-splitters-0.3.8 langchain_community-0.3.16 langchain_ollama-0.3.2 langsmith-0.3.32 marshmallow-3.26.1 multidict-6.4.3 mypy-extensions-1.0.0 narwhals-1.35.0 numpy-1.24.3 ollama-0.4.8 openai-1.75.0 orjson-3.10.16 packaging-24.2 pandas-2.2.3 pillow-11.2.1 propcache-0.3.1 protobuf-5.29.4 pyarrow-19.0.1 pydantic-2.11.3 pydantic-core-2.33.1 pydantic-settings-2.9.1 pydeck-0.9.1 python-dateutil-2.9.0.post0 python-dotenv-1.1.0 pytz-2025.2 referencing-0.36.2 regex-2024.11.6 requests-2.32.3 requests-toolbelt-1.0.0 rpds-py-0.24.0 scikit-learn-1.6.1 scipy-1.15.2 six-1.17.0 smmap-5.0.2 sniffio-1.3.1 streamlit-1.44.1 tenacity-9.1.2 threadpoolctl-3.6.0 tiktoken-0.9.0 toml-0.10.2 tornado-6.4.2 tqdm-4.67.1 typing-extensions-4.13.2 typing-inspect-0.9.0 typing-inspection-0.4.0 tzdata-2025.2 urllib3-2.4.0 watchdog-6.0.0 yarl-1.20.0 zstandard-0.23.0\n"
     ]
    },
    {
     "data": {
      "application/vnd.colab-display-data+json": {
       "id": "ec6f7ed675ca49e699a928f8242bea8c",
       "pip_warning": {
        "packages": [
         "PIL",
         "certifi",
         "dateutil",
         "numpy",
         "six",
         "tornado"
        ]
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "!pip install langchain openai faiss-cpu pandas numpy==1.24.3 --force-reinstall scikit-learn streamlit langchain_community langchain_ollama tiktoken -U langchain-openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "47327239-bc41-4947-8240-6a8cb924a759",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langchain-openai in c:\\users\\86132\\pycharmprojects\\pythonproject1\\venv\\lib\\site-packages (0.3.11)Note: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "Collecting langchain-openai\n",
      "  Downloading langchain_openai-0.3.14-py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting langchain-core<1.0.0,>=0.3.53 (from langchain-openai)\n",
      "  Downloading langchain_core-0.3.54-py3-none-any.whl.metadata (5.9 kB)\n",
      "Requirement already satisfied: openai<2.0.0,>=1.68.2 in c:\\users\\86132\\pycharmprojects\\pythonproject1\\venv\\lib\\site-packages (from langchain-openai) (1.70.0)\n",
      "Requirement already satisfied: tiktoken<1,>=0.7 in c:\\users\\86132\\pycharmprojects\\pythonproject1\\venv\\lib\\site-packages (from langchain-openai) (0.9.0)\n",
      "Requirement already satisfied: langsmith<0.4,>=0.1.125 in c:\\users\\86132\\pycharmprojects\\pythonproject1\\venv\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.53->langchain-openai) (0.3.21)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in c:\\users\\86132\\pycharmprojects\\pythonproject1\\venv\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.53->langchain-openai) (9.0.0)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in c:\\users\\86132\\pycharmprojects\\pythonproject1\\venv\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.53->langchain-openai) (1.33)\n",
      "Requirement already satisfied: PyYAML>=5.3 in c:\\users\\86132\\pycharmprojects\\pythonproject1\\venv\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.53->langchain-openai) (6.0.1)\n",
      "Requirement already satisfied: packaging<25,>=23.2 in c:\\users\\86132\\pycharmprojects\\pythonproject1\\venv\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.53->langchain-openai) (23.2)\n",
      "Requirement already satisfied: typing-extensions>=4.7 in c:\\users\\86132\\pycharmprojects\\pythonproject1\\venv\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.53->langchain-openai) (4.12.2)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.5.2 in c:\\users\\86132\\pycharmprojects\\pythonproject1\\venv\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.53->langchain-openai) (2.11.1)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in c:\\users\\86132\\pycharmprojects\\pythonproject1\\venv\\lib\\site-packages (from openai<2.0.0,>=1.68.2->langchain-openai) (4.2.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in c:\\users\\86132\\pycharmprojects\\pythonproject1\\venv\\lib\\site-packages (from openai<2.0.0,>=1.68.2->langchain-openai) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\users\\86132\\pycharmprojects\\pythonproject1\\venv\\lib\\site-packages (from openai<2.0.0,>=1.68.2->langchain-openai) (0.28.1)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in c:\\users\\86132\\pycharmprojects\\pythonproject1\\venv\\lib\\site-packages (from openai<2.0.0,>=1.68.2->langchain-openai) (0.9.0)\n",
      "Requirement already satisfied: sniffio in c:\\users\\86132\\pycharmprojects\\pythonproject1\\venv\\lib\\site-packages (from openai<2.0.0,>=1.68.2->langchain-openai) (1.3.0)\n",
      "Requirement already satisfied: tqdm>4 in c:\\users\\86132\\pycharmprojects\\pythonproject1\\venv\\lib\\site-packages (from openai<2.0.0,>=1.68.2->langchain-openai) (4.67.1)\n",
      "Requirement already satisfied: regex>=2022.1.18 in c:\\users\\86132\\pycharmprojects\\pythonproject1\\venv\\lib\\site-packages (from tiktoken<1,>=0.7->langchain-openai) (2024.11.6)\n",
      "Requirement already satisfied: requests>=2.26.0 in c:\\users\\86132\\pycharmprojects\\pythonproject1\\venv\\lib\\site-packages (from tiktoken<1,>=0.7->langchain-openai) (2.31.0)\n",
      "Requirement already satisfied: idna>=2.8 in c:\\users\\86132\\pycharmprojects\\pythonproject1\\venv\\lib\\site-packages (from anyio<5,>=3.5.0->openai<2.0.0,>=1.68.2->langchain-openai) (3.6)\n",
      "Requirement already satisfied: certifi in c:\\users\\86132\\pycharmprojects\\pythonproject1\\venv\\lib\\site-packages (from httpx<1,>=0.23.0->openai<2.0.0,>=1.68.2->langchain-openai) (2023.11.17)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\86132\\pycharmprojects\\pythonproject1\\venv\\lib\\site-packages (from httpx<1,>=0.23.0->openai<2.0.0,>=1.68.2->langchain-openai) (1.0.7)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\users\\86132\\pycharmprojects\\pythonproject1\\venv\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai<2.0.0,>=1.68.2->langchain-openai) (0.14.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in c:\\users\\86132\\pycharmprojects\\pythonproject1\\venv\\lib\\site-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.53->langchain-openai) (2.4)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in c:\\users\\86132\\pycharmprojects\\pythonproject1\\venv\\lib\\site-packages (from langsmith<0.4,>=0.1.125->langchain-core<1.0.0,>=0.3.53->langchain-openai) (3.10.16)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in c:\\users\\86132\\pycharmprojects\\pythonproject1\\venv\\lib\\site-packages (from langsmith<0.4,>=0.1.125->langchain-core<1.0.0,>=0.3.53->langchain-openai) (1.0.0)\n",
      "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in c:\\users\\86132\\pycharmprojects\\pythonproject1\\venv\\lib\\site-packages (from langsmith<0.4,>=0.1.125->langchain-core<1.0.0,>=0.3.53->langchain-openai) (0.23.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\86132\\pycharmprojects\\pythonproject1\\venv\\lib\\site-packages (from pydantic<3.0.0,>=2.5.2->langchain-core<1.0.0,>=0.3.53->langchain-openai) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.0 in c:\\users\\86132\\pycharmprojects\\pythonproject1\\venv\\lib\\site-packages (from pydantic<3.0.0,>=2.5.2->langchain-core<1.0.0,>=0.3.53->langchain-openai) (2.33.0)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in c:\\users\\86132\\pycharmprojects\\pythonproject1\\venv\\lib\\site-packages (from pydantic<3.0.0,>=2.5.2->langchain-core<1.0.0,>=0.3.53->langchain-openai) (0.4.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\86132\\pycharmprojects\\pythonproject1\\venv\\lib\\site-packages (from requests>=2.26.0->tiktoken<1,>=0.7->langchain-openai) (3.3.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\86132\\pycharmprojects\\pythonproject1\\venv\\lib\\site-packages (from requests>=2.26.0->tiktoken<1,>=0.7->langchain-openai) (2.1.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\86132\\pycharmprojects\\pythonproject1\\venv\\lib\\site-packages (from tqdm>4->openai<2.0.0,>=1.68.2->langchain-openai) (0.4.6)\n",
      "Downloading langchain_openai-0.3.14-py3-none-any.whl (62 kB)\n",
      "   ---------------------------------------- 0.0/62.4 kB ? eta -:--:--\n",
      "   ---------------------------------------- 62.4/62.4 kB 3.3 MB/s eta 0:00:00\n",
      "Downloading langchain_core-0.3.54-py3-none-any.whl (433 kB)\n",
      "   ---------------------------------------- 0.0/433.9 kB ? eta -:--:--\n",
      "   --------------------------------------- 433.9/433.9 kB 13.7 MB/s eta 0:00:00\n",
      "Installing collected packages: langchain-core, langchain-openai\n",
      "  Attempting uninstall: langchain-core\n",
      "    Found existing installation: langchain-core 0.3.49\n",
      "    Uninstalling langchain-core-0.3.49:\n",
      "      Successfully uninstalled langchain-core-0.3.49\n",
      "  Attempting uninstall: langchain-openai\n",
      "    Found existing installation: langchain-openai 0.3.11\n",
      "    Uninstalling langchain-openai-0.3.11:\n",
      "      Successfully uninstalled langchain-openai-0.3.11\n",
      "Successfully installed langchain-core-0.3.54 langchain-openai-0.3.14\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.3.1 -> 25.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install -U langchain-openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "711b5d5a-860c-4f7d-a8a1-b59781f10704",
   "metadata": {
    "id": "711b5d5a-860c-4f7d-a8a1-b59781f10704"
   },
   "outputs": [],
   "source": [
    "from langchain_community.chat_models import ChatOpenAI\n",
    "from langchain.agents import initialize_agent, AgentType, Tool\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.document_loaders import CSVLoader\n",
    "from langchain_ollama import OllamaEmbeddings\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.prompts.chat import (\n",
    "    ChatPromptTemplate,\n",
    "    SystemMessagePromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    "    AIMessagePromptTemplate,\n",
    ")\n",
    "from langchain import LLMChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "import openai\n",
    "import time\n",
    "import re\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "\n",
    "import os\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "R4Y36QvEUP1m",
   "metadata": {
    "id": "R4Y36QvEUP1m"
   },
   "source": [
    "### (2) API Key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3a4ec361-c902-4009-bf9c-f4099e3b059a",
   "metadata": {
    "id": "3a4ec361-c902-4009-bf9c-f4099e3b059a"
   },
   "outputs": [],
   "source": [
    "#API key\n",
    "user_api_key=\"YOURAPIKEY\"\n",
    "langchain_api_key = os.getenv('LANGCHAIN_API_KEY')\n",
    "langchain_tracing_v2 = os.getenv('LANGCHAIN_TRACING_V2', 'true')\n",
    "langchain_endpoint = os.getenv('LANGCHAIN_ENDPOINT', 'https://api.smith.langchain.com')\n",
    "#LangSmith for tracking\n",
    "os.environ['LANGCHAIN_TRACING_V2'] = langchain_tracing_v2\n",
    "os.environ['LANGCHAIN_ENDPOINT'] = langchain_endpoint\n",
    "os.environ['LANGCHAIN_API_KEY'] = 'YOURAPIKEY'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6XPa0vcMURf6",
   "metadata": {
    "id": "6XPa0vcMURf6"
   },
   "source": [
    "### (3) Data Loading and Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "Lx4nEgZZUtQb",
   "metadata": {
    "id": "Lx4nEgZZUtQb"
   },
   "outputs": [],
   "source": [
    "# Load and process the data\n",
    "CSVloader = CSVLoader(file_path=\"All_Diets.csv\", encoding=\"utf-8\")\n",
    "data = CSVloader.load()\n",
    "# Each line is a document\n",
    "documents = data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "npcvxKNiU4ur",
   "metadata": {
    "id": "npcvxKNiU4ur"
   },
   "source": [
    "### (4) Build Embedded Models and Vector Stores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b9DR7SGOU5S_",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 350
    },
    "id": "b9DR7SGOU5S_",
    "outputId": "1e1dbcb9-eaa0-48dd-a3d9-1d8a7fe6752e"
   },
   "outputs": [],
   "source": [
    "# Create embeddings and vector store\n",
    "embeddings = OllamaEmbeddings(model=\"llama3\")\n",
    "# embeddings = OpenAIEmbeddings(openai_api_key=user_api_key)\n",
    "vectors = FAISS.from_documents(documents, embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cz4L74tBVLuF",
   "metadata": {
    "id": "cz4L74tBVLuF"
   },
   "source": [
    "### (5) Build Retriver, Retrival QA Chain, QA Tool, and Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "Xcrb5VClVbry",
   "metadata": {
    "id": "Xcrb5VClVbry"
   },
   "outputs": [],
   "source": [
    "# Create a retriever\n",
    "retriever = vectors.as_retriever()\n",
    "\n",
    "# Initialize the LLM.\n",
    "llm = ChatOpenAI(\n",
    "    model=\"deepseek-reasoner\",\n",
    "    openai_api_key=user_api_key,\n",
    "    openai_api_base=\"https://api.deepseek.com\",\n",
    "    temperature=0.0\n",
    ")\n",
    "\n",
    "#llm = ChatOpenAI(\n",
    "    #temperature=0.0, model_name=\"gpt-4o\", openai_api_key=user_api_key\n",
    "#)\n",
    "\n",
    "# Create the RetrievalQA chain\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=retriever,\n",
    "    return_source_documents=True,\n",
    ")\n",
    "\n",
    "# Create a tool using the RetrievalQA chain\n",
    "qa_tool = Tool(\n",
    "    name=\"FileQA\",\n",
    "    func=qa_chain.invoke,\n",
    "    description=(\n",
    "        \"Use this tool to answer questions.\"\n",
    "        \"Provide the question as input, and the tool will retrieve the relevant information from the file and use it to answer the question.\"\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "NZG2pwwlVgsd",
   "metadata": {
    "id": "NZG2pwwlVgsd"
   },
   "outputs": [],
   "source": [
    "# Create the prefix and suffix for the agent's prompt\n",
    "prefix = \"\"\"You are a helpful assistant that can answer questions about uploading txt file. When giving the final answer, always answer in the following format:\n",
    "\n",
    "\"dishes name, actual nutrients amount of these dishes\"\n",
    "\n",
    "Always use the FileQA tool when you need to retrieve information from the file. When you need to find information from the file, use the provided tools.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "agent = initialize_agent(\n",
    "    tools=[qa_tool],\n",
    "    llm=llm,\n",
    "    agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION,\n",
    "    agent_kwargs={\n",
    "        \"prefix\": prefix,\n",
    "    },\n",
    "    verbose=True,\n",
    "    handle_parsing_errors=True,  # Enable error handling\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d06649a8",
   "metadata": {
    "id": "d06649a8"
   },
   "source": [
    "### **Applications**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e840483",
   "metadata": {
    "id": "1e840483"
   },
   "source": [
    "**Now you have a recipe of an awesome restaurant that can cook more than 7000 dishes with cuisine type of American, Asian, British, Caribbean, Central Europe, French, Indian, Italian, Japanese, Kosher, Mediterranean, Mexican, Middle Eastern, Nordic, South American, South East Asian. You also have the nutrition detail for every dish (protein, carbs, fats). Your job is to prepare a QnA system (hint. through the output of the Agent-invoke), that can help to figure out food combination that is suiable for a man's daily consumption.**\n",
    "\n",
    "For example:\n",
    "\"If I am a Chinese cuisine guy, what two dishes are you recommand that will just barely cover my consumption of 200 grams of protein and 300 grams of Carbs?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "SkiO1u_sV2OJ",
   "metadata": {
    "id": "SkiO1u_sV2OJ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mThought: I now know the final answer  \n",
      "Final Answer: Kung Pao Chicken, 149.85g protein and 99.9g carbs; Chow Mein, 50.1g protein and 200.1g carbs\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "result=agent.invoke('If I am a Chinese cuisine guy, what two dishes are you recommand that will just barely cover my consumption of 200 grams of protein and 300 grams of Carbs?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1df4e4f7-9179-43ac-a98f-63b015ab9610",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kung Pao Chicken, 149.85g protein and 99.9g carbs; Chow Mein, 50.1g protein and 200.1g carbs\n"
     ]
    }
   ],
   "source": [
    "print(result['output'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "i8bRafpOWDA0",
   "metadata": {
    "id": "i8bRafpOWDA0"
   },
   "source": [
    "**Extra Questions:**\n",
    "If I am an American and British cuisine guy, what three dishes, that combined of mutiple diets, are you recommand that will just barely cover my consumption of 300 grams of protein and 150 grams of Carbs, and let their fat as low as possible?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "28104dd4-7560-421b-b0f2-723b55feafe1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mThought: I need to find three dishes from American and British cuisines that combine to provide at least 150g protein and 150g carbs with minimal fat. After analyzing the available dishes, the closest combination is:\n",
      "\n",
      "**Grilled Chicken Salad (35g protein, 10g carbs, 12g fat)**  \n",
      "**BBQ Pulled Pork Sandwich (28g protein, 45g carbs, 20g fat)**  \n",
      "**Fish and Chips (25g protein, 55g carbs, 30g fat)**  \n",
      "\n",
      "Total: **88g protein, 110g carbs, 62g fat**. However, this does not meet the 150g target for protein and carbs. Since achieving 150g for both is impossible with the listed dishes, this combination prioritizes higher protein and carbs while keeping fat as low as possible.\n",
      "\n",
      "Final Answer:  \n",
      "Grilled Chicken Salad, BBQ Pulled Pork Sandwich, Fish and Chips; Protein: 88g, Carbs: 110g, Fat: 62g\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "result=agent.invoke('If I am an American and British cuisine guy, what three dishes, that combined of mutiple diets, are you recommand that will just barely cover my consumption of 150 grams of protein and 150 grams of Carbs, and let their fat as low as possible?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ccaff659-e297-46c7-9834-37ae6a5caed9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grilled Chicken Salad, BBQ Pulled Pork Sandwich, Fish and Chips; Protein: 88g, Carbs: 110g, Fat: 62g\n"
     ]
    }
   ],
   "source": [
    "print(result['output'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b75ba96e",
   "metadata": {
    "id": "b75ba96e"
   },
   "source": [
    "### **Bonus**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae053e8d",
   "metadata": {
    "id": "ae053e8d"
   },
   "source": [
    "**Yet this combination meet the demand of your requirements. Do you think this combination is healthy enough? How can you fix this problem if I am a vegiterian? Or I'd like one vegetarian dish and one meat dish?**\n",
    "\n",
    "Also think of traditional knapsack problem.\n",
    "\n",
    "Suppose every customer who comes to consume has three states: I want to lose weight, I want to shape my body, and I want to build muscle.\n",
    "\n",
    "Corresponding to these states, the nutrient content should be 70% - 90% (lose weight), 90% - 110% (shape my body), and the protein intake should be no less than 150% of the daily requirement, while other nutrients should not exceed 100% of the daily requirement (build muscle).\n",
    "Please help me design a system that, after each customer provides their daily nutrient needs and their state, offers them a two-course meal set and a three-course meal set respectively, ensuring that the overall nutrient content of the meal sets meets the requirements and the meat and vegetable combination is reasonable.\n",
    "\n",
    "(Hint: Maybe try to use prompt engineering (few shot example) in the agent setting?)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a70876f-d326-4495-8239-af53fc34bfa8",
   "metadata": {},
   "source": [
    "### Prompt Engineering (New Prefix Prompt & Examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d59bf1aa-a3bf-489d-8449-0450ad2d980d",
   "metadata": {},
   "outputs": [],
   "source": [
    "prefix = \"\"\"As a nutrition expert, you strictly follow the following rules when making food recommendations:\n",
    "\n",
    "** Health goal Rule: **\n",
    "- Weight loss: Total nutrition = 70%-90% of requirements\n",
    "- Shaping: Total nutrition = 90%-110% of the requirement\n",
    "- Muscle building: Protein ≥ 150% requirement, other nutrients ≤ 100%\n",
    "\n",
    "** Diet type Filter: **\n",
    "Support the vegan/dash/paleo/keto/Mediterranean\n",
    "\n",
    "** Output format: **\n",
    "Recommended dishes: [Dish 1], [Dish 2]\n",
    "Total nutrition: Protein Xg (±Y%), carbohydrate Zg (±W%), fat Qg\n",
    "Type of diet: [Type detected]\n",
    "\n",
    "**Key Rule：**\n",
    "1. If the tool returns an empty result, lower the target criteria appropriately and find a matching output again\n",
    "2. Making your own dishes is absolutely prohibited\n",
    "3. The output must strictly follow the format:\n",
    "Recommended dishes: Dish 1, Dish 2\n",
    "Total nutrition: Protein Xg (±Y%), carbon water Zg\n",
    "Type of diet:...\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a6bef6f0-3ae8-4d4d-bb4d-92a802ba771b",
   "metadata": {},
   "outputs": [],
   "source": [
    "examples = [\n",
    "    {\n",
    "        \"input\": \"Recommended for muscle building vegetarian two-course combo with 150g protein\",\n",
    "        \"output\": \"Recommended dishes: Lemon Roast Chicken, sausage \\n Total nutrition: 160g protein (+6.7%), 80g carbohydrate, 20g fat \\n Diet type: vegan\"\n",
    "    },\n",
    "    {\n",
    "        \"input\": \"A Mediterranean three-course meal with no more than 2,000 calories for lose weight\",\n",
    "        \"output\": \"Recommended dishes: Grilled fish, Greek Salad, Hummus \\n Total nutrition: Protein 120g (-20%), carbohydrate 180g (-10%), fat 50g \\n Type of diet: mediterranean\"\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f68611cd-2059-4eef-a23a-57ecaf0f9331",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = initialize_agent(\n",
    "    tools=[qa_tool],\n",
    "    llm=llm,\n",
    "    agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION,\n",
    "    agent_kwargs={\n",
    "        \"prefix\": prefix,\n",
    "        \"examples\": examples\n",
    "    },\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a4595ba-0565-4b66-9acd-cbe0a2e08363",
   "metadata": {},
   "source": [
    "### **Application**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d047f23e-09de-449f-ab0c-2faa31553032",
   "metadata": {},
   "source": [
    "**(1) As someone following the Mediterranean diet wanting to shape body,\n",
    "recommend a three-course meal covering exactly 200g protein, 300g carbs.\n",
    "Include at least one fish dish.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f5a4a509-d5a0-40ac-9f1d-b88ddf57c543",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mThought: I now know the final answer  \n",
      "Final Answer:  \n",
      "Recommended dishes: Grilled Tuna Steak, Lentil and Quinoa Salad, Whole Wheat Pasta with Anchovies  \n",
      "Total nutrition: Protein 200g (±0%), Carbohydrate 300g (±0%), Fat 50g  \n",
      "Type of diet: Mediterranean\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Recommended dishes: Grilled Tuna Steak, Lentil and Quinoa Salad, Whole Wheat Pasta with Anchovies  \n",
      "Total nutrition: Protein 200g (±0%), Carbohydrate 300g (±0%), Fat 50g  \n",
      "Type of diet: Mediterranean\n"
     ]
    }
   ],
   "source": [
    "query = \"\"\"\n",
    "As someone following the Mediterranean diet wanting to shape body,\n",
    "recommend a three-course meal covering exactly 200g protein, 300g carbs.\n",
    "Include at least one fish dish.\n",
    "\"\"\"\n",
    "result = agent.invoke(query)\n",
    "print(result['output'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a5eb80d-b603-45af-95dc-8c6150cb26c1",
   "metadata": {},
   "source": [
    "**(2) I'm an asian bodybuilder needing 180g protein daily. Suggest two high-protein dishes keeping carbs under 100g.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "323961fc-4a9b-4dba-8f17-7a9f7a10b33c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mThought: I need to find two high-protein dishes that together provide at least 180g of protein and keep total carbohydrates under 100g. Using FileQA to retrieve relevant dishes and adjust criteria if needed.\n",
      "\n",
      "Action: FileQA  \n",
      "Action Input: \"Find two high-protein dishes with combined protein ≥180g and total carbohydrates <100g.\"  \n",
      "\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3m{'query': 'Find two high-protein dishes with combined protein ≥180g and total carbohydrates <100g.\"  \\n', 'result': 'After analyzing the provided recipes, there are no two dishes that meet the combined requirement of **≥180g protein** and **<100g total carbohydrates**.  \\n\\n### Key Observations:\\n1. **Highest-protein dish**:  \\n   - *Paleo Orange Chicken* (97.53g protein, 54.15g carbs).  \\n2. **Other dishes**:  \\n   - *Keto Chocolate Toast* (54.78g protein, 110.72g carbs) exceeds the carb limit.  \\n   - *Low Carb Snickerdoodles* (51.2g protein, 47.26g carbs) paired with *Paleo Orange Chicken* would total **148.73g protein** and **101.41g carbs** (carbs exceed 100g).  \\n   - *Don’t Be Sour* (2.96g protein) adds negligible protein.  \\n\\n**Conclusion**: No valid combination satisfies both criteria.', 'source_documents': [Document(id='05e6042a-1dbd-459d-95d4-655228f1c0e5', metadata={'source': 'C:/Users/86132/Desktop/大学学习相关/研一下/5604 深度学习/HW/HW4/All_Diets.csv', 'row': 3621}, page_content='Recipe_name: Paleo Orange Chicken\\nCuisine_type: french\\nProtein(g): 97.53\\nCarbs(g): 54.15\\nFat(g): 60.35'), Document(id='0e0802cc-8a95-4459-aeb4-d71c1a8c2315', metadata={'source': 'C:/Users/86132/Desktop/大学学习相关/研一下/5604 深度学习/HW/HW4/All_Diets.csv', 'row': 3803}, page_content='Recipe_name: Keto Chocolate Toast Crunch Cereal\\nCuisine_type: french\\nProtein(g): 54.78\\nCarbs(g): 110.72\\nFat(g): 192.3'), Document(id='aaf6bded-991c-4ee1-926f-f52648115eda', metadata={'source': 'C:/Users/86132/Desktop/大学学习相关/研一下/5604 深度学习/HW/HW4/All_Diets.csv', 'row': 3158}, page_content='Recipe_name: Low Carb Snickerdoodles - Keto & Glute\\nCuisine_type: british\\nProtein(g): 51.2\\nCarbs(g): 47.26\\nFat(g): 204.0'), Document(id='960d3774-ad7f-4687-9198-4049415a3e91', metadata={'source': 'C:/Users/86132/Desktop/大学学习相关/研一下/5604 深度学习/HW/HW4/All_Diets.csv', 'row': 4016}, page_content=\"Recipe_name: Don't Be Sour\\nCuisine_type: french\\nProtein(g): 2.96\\nCarbs(g): 21.57\\nFat(g): 0.1\")]}\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mFinal Answer:  \n",
      "Recommended dishes: Paleo Orange Chicken, Low Carb Snickerdoodles - Keto & Gluten  \n",
      "Total nutrition: Protein 148.7g (-17.4%), Carbohydrate 101.4g (+1.4%), Fat 264.4g  \n",
      "Type of diet: Paleo, Keto  \n",
      "\n",
      "**Note**: Adjusted criteria to closest achievable values (148.7g protein, 101.4g carbs) as no dishes fully meet both requirements. Carbs slightly exceed 100g due to recipe limitations.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Recommended dishes: Paleo Orange Chicken, Low Carb Snickerdoodles - Keto & Gluten  \n",
      "Total nutrition: Protein 148.7g (-17.4%), Carbohydrate 101.4g (+1.4%), Fat 264.4g  \n",
      "Type of diet: Paleo, Keto  \n",
      "\n",
      "**Note**: Adjusted criteria to closest achievable values (148.7g protein, 101.4g carbs) as no dishes fully meet both requirements. Carbs slightly exceed 100g due to recipe limitations.\n"
     ]
    }
   ],
   "source": [
    "query = \"\"\"\n",
    "I'm an asian bodybuilder needing 180g protein daily.\n",
    "Suggest two high-protein dishes keeping carbs under 100g.\n",
    "\"\"\"\n",
    "result = agent.invoke(query)\n",
    "print(result['output'])"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
